# -*- coding: utf-8 -*-
"""Perkiraan Tingkat Polusi DKI Jakarta dengan Random Forest Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5cWwcM8dVHAFZoZQ7JFzMoNNq8nbwpp

Setup Environment
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
!tar xf spark-3.5.1-bin-hadoop3.tgz
!pip install -q findspark

import os
os.environ ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"
!ls
import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True)
spark

"""Proses ETL(Extract, Transform, Load)"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from google.colab import drive

drive.mount('/content/drive')

# Membuat objek spark
spark = SparkSession.builder \
    .appName("PolusiData") \
    .getOrCreate()

# Path ke gdrive
path = "/content/drive/My Drive/Tugas BDA/polusi.csv"

# Load dari gdrive
df = spark.read.csv(path, header=True, inferSchema=True, sep=",")

df.describe().show(10)

# Transformasi: hapus null, ubah format tanggal
df = df.dropna()
df = df.withColumn("tanggal", to_date(col("tanggal"), "M/d/yyyy"))

# Simpan output transformasi
df.write.mode("overwrite").parquet("/content/drive/My Drive/Tugas BDA/clean_data")

# Melihat informasi statistika dari dataset
df.describe().show(10)

# Mempersiapkan dataset
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, DoubleType, IntegerType

# Transformasi Dataframe
df = df.withColumn("max", col("max").cast(DoubleType()))

# Membuat kategori nilai
def condition(r):
  if (0 <= r <= 50):
    label = "aman"
  elif (51 <= r <= 100):
    label = "sedang"
  else:
    label = "tidak sehat"
  return label

# Mengkonversi integer ke double
df = df.withColumn("pm10", col("pm10").cast(DoubleType()))
df = df.withColumn("pm25", col("pm25").cast(DoubleType()))
df = df.withColumn("so2", col("so2").cast(DoubleType()))
df = df.withColumn("co", col("co").cast(DoubleType()))
df = df.withColumn("o3", col("o3").cast(DoubleType()))
df = df.withColumn("no2", col("no2").cast(DoubleType()))

# Menerapkan UDF pada kolom 'max' dan menimpan di kolom 'quality'
string_udf = udf(condition, StringType())
df = df.withColumn('quality', string_udf(col('max')))

# Menunjukan output sebanyak 10 baris
df.show(10)

"""Preprocessing"""

# Membuat fitur dan label model ML
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml import Pipeline

# Encoding kolom 'quality'
label_indexer = StringIndexer(inputCol="quality", outputCol="label")

# Encoding kolom 'location'
location_indexer = StringIndexer(inputCol="location", outputCol="location_index", handleInvalid="keep")
location_encoder = OneHotEncoder(inputCol="location_index", outputCol="location_vec")

# Vektorisasi fitur numerik dan One-Hot Encoded
feature_assembler = VectorAssembler(inputCols=["pm10", "pm25", "so2", "co", "o3", "no2", "location_vec"], outputCol="features")

# Memberikan output
print("\nFitur yang akan digunakan: ", feature_assembler)

# Membangun pipeline ML
from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier

# Menggunakan random forest untuk membuat model klasifikasi
classifier = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=100, seed=42)

# Membuat pipeline
pipeline = Pipeline(stages=[
    location_indexer,
    location_encoder,
    label_indexer,
    feature_assembler,
    classifier
])

"""Implementasi Model Machine Learning"""

# Membagi data training dan testing set
(trainingData, testData) = df.randomSplit([0.8, 0.2], seed=1234)

print(f"\nJumlah data training: {trainingData.count()}")
print(f"Jumlah data testing: {testData.count()}")

# Melatih model
print("\nMemulai pelatihan model...")
model = pipeline.fit(trainingData)

"""Evaluasi Model Machine Learning"""

# Mengevaluasi model
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Membuat prediksi pada test data
print("\nMembuat prediksi pada data testing...")
predictions = model.transform(testData)

# Menampilkan prediksi
print("\nMenampilkan prediksi:")
predictions.show(10)

# Mengevaluasi model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")

# Hitung akurasi
accuracy = evaluator.evaluate(predictions)
print(f"\nAccuracy pada test set = {accuracy}")

# Mengevaluasi dengan matrix F1-score
f1_score = evaluator.evaluate(predictions, {evaluator.metricName: "f1"})
print(f"F1-Score pada test set = {f1_score}")

"""Analisis Data"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pyspark.ml.feature import IndexToString
from pyspark.mllib.evaluation import MulticlassMetrics

# Mengambil StringIndexerModel dari pipeline untuk mapping label
fitted_label_indexer_model = model.stages[2]
label_mapping = fitted_label_indexer_model.labels

# Mengkonversi kolom 'prediction' dari numerik ke string
converter = IndexToString(inputCol="prediction", outputCol="predicted_quality_string", labels=label_mapping)
predictions_with_strings = converter.transform(predictions)

# Menganalisis kolom
predictions_pd = predictions_with_strings.select(
    "tanggal", "max", "categori", "quality", "predicted_quality_string"
).toPandas()

print("\n--- Contoh DataFrame Pandas untuk Visualisasi ---")
print(predictions_pd.head())

"""Visualisasi Data"""

# Set style
sns.set_style("whitegrid")

# Distribusi prediksi vs label asli
plt.figure(figsize=(12, 6))
sns.countplot(data=predictions_pd, x='quality', hue='predicted_quality_string', palette='viridis')
plt.title('Distribusi Aktual vs. Prediksi Kualitas Udara')
plt.xlabel('Kualitas Udara Aktual')
plt.ylabel('Jumlah Data')
plt.legend(title='Diprediksi')
plt.show()

# Confusion matrix
predictionAndLabels = predictions.select("prediction", "label").rdd.map(lambda row: (float(row.prediction), float(row.label)))
metrics = MulticlassMetrics(predictionAndLabels)
confusion_matrix = metrics.confusionMatrix().toArray()

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix, annot=True, fmt=".0f", cmap="Blues",
            xticklabels=label_mapping, yticklabels=label_mapping)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Analisis Fitur (RandomForestClassificationModel)
from pyspark.ml.classification import RandomForestClassificationModel
import numpy as np

if isinstance(model.stages[-1], RandomForestClassificationModel):
    feature_importances = model.stages[-1].featureImportances.toArray()
    print("\nNilai Feature Importances:", feature_importances)

    assembler_model = model.stages[3]
    assembler_input_cols = assembler_model.getInputCols()

    features_and_importances = list(zip(assembler_input_cols, feature_importances))
    features_and_importances.sort(key=lambda x: x[1], reverse=True)

    if len(features_and_importances) < 10:
        num_features_to_show = len(features_and_importances)
    else:
        num_features_to_show = 10
    top_n_features = features_and_importances[:num_features_to_show]

    print("Top 10 Features and Importances (Nama Fitur, Nilai Importance):", top_n_features)

    plt.figure(figsize=(10, 6))
    sns.barplot(x=[f[1] for f in top_n_features], y=[f[0] for f in top_n_features], palette='crest')
    plt.title('10 Fitur Penting')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()

# Tren kualitas udara seiring waktu
if 'tanggal' in predictions_pd.columns:
  daily_quality_counts = predictions_pd.groupby(['tanggal', 'predicted_quality_string']).size().unstack(fill_value=0)
  daily_quality_counts = daily_quality_counts.sort_index()
  plt.figure(figsize=(15, 7))
  daily_quality_counts.plot(kind='area', stacked=True, colormap='viridis', figsize=(15, 7))
  plt.title('Tren Prediksi Kualitas Udara Harian')
  plt.xlabel('Tanggal')
  plt.ylabel('Jumlah Kejadian')
  plt.legend(title='Kualitas Udara')
  plt.tight_layout()
  plt.show()

# Menyimpan model
model = pipeline.fit(trainingData)
model_path = "/content/drive/My Drive/Tugas BDA/Model"
model.save(model_path)

spark.stop()

"""Deployment dengan Dashboard"""

# Mengakses app.py
from google.colab import drive
drive.mount('/content/drive/')

# Menginstall library yang dibutuhkan
!pip install streamlit pyspark pandas matplotlib seaborn numpy
!npm install -g localtunnel

# Menggunakan ngrok
!pip install pyngrok
from pyngrok import ngrok

# Token pertama kali saja
from pyngrok import ngrok
ngrok.set_auth_token("2xjy5N3okhdhSj5Y9Idsks7jbeE_78H3TLmnzHkz8QizWt5Lb")

# Jalankan Streamlit di background
!streamlit run "/content/drive/MyDrive/Tugas BDA/app.py" > log.txt 2>&1 &

# Tunggu beberapa detik
import time; time.sleep(10)

# Buka tunnel
public_url = ngrok.connect(addr="localhost:8501")
print("ðŸ”— App kamu: ", public_url)